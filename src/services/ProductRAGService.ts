import OpenAI from 'openai';
import { ProductVectorStoreService } from '../services/ProductVectorStoreService';
import { QueryParserService } from '../services/QueryParserService';
import { VectorItem } from '../types/rag';
import { config, openai } from '../config';
import { Logger } from '../utils/logger';

export class ProductRAGService {
  private openai: OpenAI;
  private vectorStore: ProductVectorStoreService;
  private queryParser: QueryParserService;

  constructor(items: VectorItem[]) {
    this.openai = openai;
    this.vectorStore = new ProductVectorStoreService(items);
    this.queryParser = new QueryParserService();
  }

  /**
   * Initialize RAG service
   */
  async initialize(): Promise<void> {
    Logger.info('üîÑ Initializing Product RAG service...');
    await this.vectorStore.initialize();
    Logger.success('‚úÖ Product RAG service ready');
  }

  /**
   * Search products using natural language (with automatic filter extraction)
   */
  async searchProductsNatural(query: string, limit: number = 5): Promise<{
    answer: string;
    products: Array<{
      id: string;
      title: string;
      similarity: number;
      price?: string;
      category: string;
      brand?: string;
    }>;
    tokensUsed: number;
    extractedFilters?: any;
    cleanedQuery?: string;
  }> {
    Logger.info('ü§ñ Processing natural language search:', query);

    try {
      // Parse query to extract filters automatically
      const { cleanedQuery, extractedFilters, searchFilters } = await this.queryParser.parseQuery(query);
      
      Logger.info(`üîç Using cleaned query: "${cleanedQuery}"`);
      Logger.info('üéØ Auto-extracted filters:', searchFilters);

      // Use the existing search with auto-extracted filters
      const result = await this.searchProducts(cleanedQuery, { ...searchFilters, limit });

      return {
        ...result,
        extractedFilters,
        cleanedQuery
      };
      
    } catch (error) {
      Logger.error('‚ùå Natural language search failed:', error);
      // Fallback to original query without filters
      return await this.searchProducts(query, { limit });
    }
  }
  async searchProducts(query: string, filters?: {
    category?: string;
    maxPrice?: number;
    brand?: string;
    limit?: number;
  }): Promise<{
    answer: string;
    products: Array<{
      id: string;
      title: string;
      similarity: number;
      price?: string;
      category: string;
      brand?: string;
    }>;
    tokensUsed: number;
  }> {
    Logger.info('üîç Processing product search:', query);

    const limit = filters?.limit || 3;
    const threshold = 0.4; // M√°s preciso para productos

    // 1. Search for relevant products
    let searchResults = await this.vectorStore.searchSimilar(query, limit, threshold);

    // 2. Apply filters if provided
    if (filters?.category) {
      searchResults = searchResults.filter(result => 
        result.item.category === filters.category
      );
    }

    if (filters?.maxPrice) {
      searchResults = searchResults.filter(result => 
        !result.item.price || result.item.price <= filters.maxPrice!
      );
    }

    if (filters?.brand) {
      searchResults = searchResults.filter(result => 
        result.item.brand === filters.brand
      );
    }

    // Limit final results
    searchResults = searchResults.slice(0, limit);

    if (searchResults.length === 0) {
      Logger.warn('‚ö†Ô∏è No relevant products found for query');
      return {
        answer: 'Lo siento, no encontr√© productos que coincidan con tu b√∫squeda. ¬øPodr√≠as ser m√°s espec√≠fico o probar con otros t√©rminos?',
        products: [],
        tokensUsed: 0,
      };
    }

    // 3. Build context from relevant products
    const productsContext = searchResults
      .map((result, index) =>
        `${index + 1}. **${result.item.title}** (Relevancia: ${(
          result.similarity * 100
        ).toFixed(1)}%)\n${result.item.content}\nPrecio: $${result.item.price || 'No disponible'}\nMarca: ${result.item.brand || 'N/A'}`
      )
      .join('\n\n');

    // 4. Generate contextualized response
    const systemPrompt = `Eres un experto asistente de compras. Tu trabajo es recomendar productos bas√°ndote √öNICAMENTE en la informaci√≥n proporcionada.

INSTRUCCIONES:
1. Responde de forma amigable y √∫til
2. Menciona los productos m√°s relevantes por nombre
3. Explica brevemente por qu√© cada producto es una buena opci√≥n
4. Incluye informaci√≥n de precios cuando sea relevante
5. Si la consulta no est√° bien cubierta, sugiere alternativas
6. Mant√©n un tono conversacional pero profesional

PRODUCTOS ENCONTRADOS:
${productsContext}

Responde como si fueras un vendedor experto recomendando estos productos espec√≠ficos.`;

    const completion = await this.openai.chat.completions.create({
      model: config.openai.model,
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: query },
      ],
      temperature: 0.4, // Un poco m√°s creativo para recomendaciones
      max_tokens: 200,
    });

    const answer = completion.choices[0].message.content || 
      'No se pudo generar una recomendaci√≥n.';
    const tokensUsed = completion.usage?.total_tokens || 0;

    Logger.success(`‚úÖ Product search processed (${tokensUsed} tokens used)`);

    return {
      answer,
      products: searchResults.map((result) => ({
        id: result.item.id,
        title: result.item.title,
        similarity: result.similarity,
        category: result.item.category,
        brand: result.item.brand,
        price: result.item.price ? `$${result.item.price}` : undefined,
      })),
      tokensUsed,
    };
  }

  /**
   * Get vector store instance (for debugging/testing)
   */
  getVectorStore(): ProductVectorStoreService {
    return this.vectorStore;
  }
}